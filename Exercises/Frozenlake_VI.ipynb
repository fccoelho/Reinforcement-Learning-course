{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Frozenlake Value Iteration exercise\n",
    "Below you can find an implementation for solving the Frozenlake problem using the Value Iteration algorithm. \n",
    "**Tasks:**\n",
    "1. After running the code to verify that the agent can solve the problem, you can run the code below to visualize the agent playing the game, using the optimal policy found by the Value Iteration algorithm.\n",
    "2. Make a plot with the evolution of the reward obtained by the agent during the training process.\n",
    "3. Create a visualization of the optimal policy found by the agent. You can do it similarly to what was done for the gridworld problem in the previous chapter."
   ],
   "id": "2e246b4b16608ae8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-04T14:40:47.583380Z",
     "start_time": "2024-09-04T14:40:44.890685Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T14:40:47.595307Z",
     "start_time": "2024-09-04T14:40:47.590838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 5"
   ],
   "id": "104f34e93f6af108",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T14:40:47.755348Z",
     "start_time": "2024-09-04T14:40:47.745207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Agent:\n",
    "    def __init__(self, render=None):\n",
    "        self.env = gym.make(ENV_NAME, render_mode=render)\n",
    "        self.state, info = self.env.reset()\n",
    "        self.values = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "    \n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _, info = self.env.step(action)\n",
    "            self.transits[self.state, action][new_state] += 1\n",
    "            self.rewards[self.state, action, new_state] = reward\n",
    "            self.state, info = self.env.reset() if is_done else (new_state, {})\n",
    "    \n",
    "    def calc_action_value(self, state, action):\n",
    "        target_counts = self.transits[state, action]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            reward = self.rewards[state, action, tgt_state]\n",
    "            val = reward + GAMMA * self.values[tgt_state]\n",
    "            action_value += (count / total) * val\n",
    "        return action_value\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or (best_value < action_value):\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "    \n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state, info = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _, info = env.step(action)\n",
    "            self.transits[state, action][new_state] += 1\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "    \n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [self.calc_action_value(state, action) for action in range(self.env.action_space.n)]\n",
    "            self.values[state] = max(state_values)\n",
    "            "
   ],
   "id": "a4f76a0f3f9b9821",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T14:40:57.840630Z",
     "start_time": "2024-09-04T14:40:57.443381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_env = gym.make(ENV_NAME, render_mode=None)\n",
    "agent = Agent()\n",
    "writer = SummaryWriter(comment=\"-v-iteration\")\n",
    "iter_no = 0\n",
    "best_reward = 0.0\n",
    "while True:\n",
    "    iter_no += 1\n",
    "    agent.play_n_random_steps(100)\n",
    "    agent.value_iteration()\n",
    "    \n",
    "    reward = 0.0\n",
    "    for _ in range(TEST_EPISODES):    \n",
    "        reward += agent.play_episode(test_env)\n",
    "        print(f\"Iteration {iter_no} - Test run {_}, reward: {reward}\\r\", end=\"\" if _< TEST_EPISODES-1 else \"\\n\")\n",
    "    reward /= TEST_EPISODES\n",
    "    writer.add_scalar(\"reward\", reward, iter_no)\n",
    "    if reward > best_reward:\n",
    "        print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "        best_reward = reward\n",
    "    if reward > 0.80:\n",
    "        print(f\"Solved in {iter_no} iterations!\")\n",
    "        break\n",
    "writer.close()"
   ],
   "id": "1eb9094f71cf484c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Test run 4, reward: 0.0\r\n",
      "Iteration 2 - Test run 4, reward: 0.0\r\n",
      "Iteration 3 - Test run 4, reward: 0.0\r\n",
      "Iteration 4 - Test run 4, reward: 0.0\r\n",
      "Iteration 5 - Test run 4, reward: 0.0\r\n",
      "Iteration 6 - Test run 4, reward: 0.0\r\n",
      "Iteration 7 - Test run 4, reward: 0.0\r\n",
      "Iteration 8 - Test run 4, reward: 0.0\r\n",
      "Iteration 9 - Test run 4, reward: 0.0\r\n",
      "Iteration 10 - Test run 4, reward: 0.0\r\n",
      "Iteration 11 - Test run 4, reward: 0.0\r\n",
      "Iteration 12 - Test run 4, reward: 0.0\r\n",
      "Iteration 13 - Test run 4, reward: 1.0\r\n",
      "Best reward updated 0.000 -> 0.200\n",
      "Iteration 14 - Test run 4, reward: 0.0\r\n",
      "Iteration 15 - Test run 4, reward: 1.0\r\n",
      "Iteration 16 - Test run 4, reward: 0.0\r\n",
      "Iteration 17 - Test run 4, reward: 2.0\r\n",
      "Best reward updated 0.200 -> 0.400\n",
      "Iteration 18 - Test run 4, reward: 1.0\r\n",
      "Iteration 19 - Test run 4, reward: 1.0\r\n",
      "Iteration 20 - Test run 4, reward: 4.0\r\n",
      "Best reward updated 0.400 -> 0.800\n",
      "Iteration 21 - Test run 4, reward: 0.0\r\n",
      "Iteration 22 - Test run 4, reward: 3.0\r\n",
      "Iteration 23 - Test run 4, reward: 1.0\r\n",
      "Iteration 24 - Test run 4, reward: 1.0\r\n",
      "Iteration 25 - Test run 4, reward: 2.0\r\n",
      "Iteration 26 - Test run 4, reward: 0.0\r\n",
      "Iteration 27 - Test run 4, reward: 4.0\r\n",
      "Iteration 28 - Test run 4, reward: 3.0\r\n",
      "Iteration 29 - Test run 4, reward: 4.0\r\n",
      "Iteration 30 - Test run 4, reward: 3.0\r\n",
      "Iteration 31 - Test run 4, reward: 3.0\r\n",
      "Iteration 32 - Test run 4, reward: 1.0\r\n",
      "Iteration 33 - Test run 4, reward: 2.0\r\n",
      "Iteration 34 - Test run 4, reward: 2.0\r\n",
      "Iteration 35 - Test run 4, reward: 4.0\r\n",
      "Iteration 36 - Test run 4, reward: 3.0\r\n",
      "Iteration 37 - Test run 4, reward: 4.0\r\n",
      "Iteration 38 - Test run 4, reward: 2.0\r\n",
      "Iteration 39 - Test run 4, reward: 2.0\r\n",
      "Iteration 40 - Test run 4, reward: 4.0\r\n",
      "Iteration 41 - Test run 4, reward: 2.0\r\n",
      "Iteration 42 - Test run 4, reward: 5.0\r\n",
      "Best reward updated 0.800 -> 1.000\n",
      "Solved in 42 iterations!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eaf484f9d41aa40d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

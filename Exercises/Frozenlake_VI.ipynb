{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Frozenlake Value Iteration exercise\n",
    "Below you can find an implementation for solving the Frozenlake problem using the Value Iteration algorithm. \n",
    " \n",
    "**Tasks:**\n",
    "1. After running the code to verify that the agent can solve the problem, you can run the code below to visualize the agent playing the game, using the optimal policy found by the Value Iteration algorithm.\n",
    "2. Make a plot with the evolution of the reward obtained by the agent during the training process.\n",
    "3. Create a visualization of the optimal policy found by the agent. You can do it similarly to what was done for the gridworld problem in the previous chapter."
   ],
   "id": "2e246b4b16608ae8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-23T16:33:46.138409Z",
     "start_time": "2025-09-23T16:33:45.997905Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T16:33:47.115672Z",
     "start_time": "2025-09-23T16:33:47.112309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20"
   ],
   "id": "104f34e93f6af108",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T16:33:48.979554Z",
     "start_time": "2025-09-23T16:33:48.968486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Agent:\n",
    "    def __init__(self, render=None):\n",
    "        self.env = gym.make(ENV_NAME, render_mode=render)\n",
    "        self.state, info = self.env.reset()\n",
    "        self.values = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "    \n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _, info = self.env.step(action)\n",
    "            self.transits[self.state, action][new_state] += 1\n",
    "            self.rewards[self.state, action, new_state] = reward\n",
    "            self.state, info = self.env.reset() if is_done else (new_state, {})\n",
    "    \n",
    "    def calc_action_value(self, state, action):\n",
    "        target_counts = self.transits[state, action]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            reward = self.rewards[state, action, tgt_state]\n",
    "            val = reward + GAMMA * self.values[tgt_state]\n",
    "            action_value += (count / total) * val\n",
    "        return action_value\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or (best_value < action_value):\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "    \n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state, info = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _, info = env.step(action)\n",
    "            self.transits[state, action][new_state] += 1\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "    \n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [self.calc_action_value(state, action) for action in range(self.env.action_space.n)]\n",
    "            self.values[state] = max(state_values)\n",
    "            "
   ],
   "id": "a4f76a0f3f9b9821",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T16:34:36.897928Z",
     "start_time": "2025-09-23T16:34:22.383593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_env = gym.make(ENV_NAME, render_mode=\"human\")\n",
    "test_env.metadata['render_fps']=0 # Velocidade máxima de renderização\n",
    "agent = Agent()\n",
    "writer = SummaryWriter(comment=\"-v-iteration\")\n",
    "iter_no = 0\n",
    "best_reward = 0.0\n",
    "while True:\n",
    "    iter_no += 1\n",
    "    agent.play_n_random_steps(100)\n",
    "    agent.value_iteration()\n",
    "    \n",
    "    reward = 0.0\n",
    "    for _ in range(TEST_EPISODES):    \n",
    "        reward += agent.play_episode(test_env)\n",
    "        print(f\"Iteration {iter_no} - Test run {_}, reward: {reward}\\r\", end=\"\" if _< TEST_EPISODES-1 else \"\\n\")\n",
    "    reward /= TEST_EPISODES\n",
    "    writer.add_scalar(\"reward\", reward, iter_no)\n",
    "    if reward > best_reward:\n",
    "        print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "        best_reward = reward\n",
    "    if reward > 0.80:\n",
    "        print(f\"Solved in {iter_no} iterations!\")\n",
    "        break\n",
    "writer.close()"
   ],
   "id": "1eb9094f71cf484c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Datos/MEGAsync/Cursos/Reinforcement_Learning/Reinforcement-Learning-course/.venv/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Test run 19, reward: 0.0\r\n",
      "Iteration 2 - Test run 19, reward: 0.0\r\n",
      "Iteration 3 - Test run 19, reward: 0.0\r\n",
      "Iteration 4 - Test run 19, reward: 0.0\r\n",
      "Iteration 5 - Test run 19, reward: 0.0\r\n",
      "Iteration 6 - Test run 19, reward: 0.0\r\n",
      "Iteration 7 - Test run 19, reward: 0.0\r\n",
      "Iteration 8 - Test run 19, reward: 0.0\r\n",
      "Iteration 9 - Test run 19, reward: 0.0\r\n",
      "Iteration 10 - Test run 19, reward: 0.0\r\n",
      "Iteration 11 - Test run 19, reward: 0.0\r\n",
      "Iteration 12 - Test run 19, reward: 0.0\r\n",
      "Iteration 13 - Test run 19, reward: 0.0\r\n",
      "Iteration 14 - Test run 19, reward: 0.0\r\n",
      "Iteration 15 - Test run 19, reward: 0.0\r\n",
      "Iteration 16 - Test run 19, reward: 0.0\r\n",
      "Iteration 17 - Test run 19, reward: 0.0\r\n",
      "Iteration 18 - Test run 19, reward: 0.0\r\n",
      "Iteration 19 - Test run 19, reward: 3.0\r\n",
      "Best reward updated 0.000 -> 0.150\n",
      "Iteration 20 - Test run 19, reward: 0.0\r\n",
      "Iteration 21 - Test run 19, reward: 7.0\r\n",
      "Best reward updated 0.150 -> 0.350\n",
      "Iteration 22 - Test run 19, reward: 12.0\r\n",
      "Best reward updated 0.350 -> 0.600\n",
      "Iteration 23 - Test run 19, reward: 12.0\r\n",
      "Iteration 24 - Test run 19, reward: 11.0\r\n",
      "Iteration 25 - Test run 19, reward: 12.0\r\n",
      "Iteration 26 - Test run 19, reward: 10.0\r\n",
      "Iteration 27 - Test run 19, reward: 11.0\r\n",
      "Iteration 28 - Test run 19, reward: 10.0\r\n",
      "Iteration 29 - Test run 19, reward: 15.0\r\n",
      "Best reward updated 0.600 -> 0.750\n",
      "Iteration 30 - Test run 19, reward: 14.0\r\n",
      "Iteration 31 - Test run 19, reward: 14.0\r\n",
      "Iteration 32 - Test run 19, reward: 17.0\r\n",
      "Best reward updated 0.750 -> 0.850\n",
      "Solved in 32 iterations!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eaf484f9d41aa40d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

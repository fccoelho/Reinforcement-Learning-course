{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Rendering Cliff Walking\n",
    "WARNING: this code is still incomplete."
   ],
   "id": "17873731ea54a605"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-16T16:20:20.605109Z",
     "start_time": "2024-09-16T16:20:20.478110Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "# gym.make('CliffWalking-v0')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T16:20:22.580265Z",
     "start_time": "2024-09-16T16:20:22.573535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# world height\n",
    "WORLD_HEIGHT = 4\n",
    "\n",
    "# world width\n",
    "WORLD_WIDTH = 12\n",
    "\n",
    "# probability for exploration\n",
    "EPSILON = 0.1\n",
    "\n",
    "# step size\n",
    "ALPHA = 0.5\n",
    "\n",
    "# gamma for Q-Learning and Expected Sarsa\n",
    "GAMMA = 1\n",
    "\n",
    "# all possible actions\n",
    "ACTION_UP = 0\n",
    "ACTION_DOWN = 1\n",
    "ACTION_LEFT = 2\n",
    "ACTION_RIGHT = 3\n",
    "ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
    "\n",
    "# initial state action pair values\n",
    "START = [3, 0]\n",
    "GOAL = [3, 11]"
   ],
   "id": "5d5fbef86e41e73b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T16:21:24.354794Z",
     "start_time": "2024-09-16T16:21:24.346819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def step(state, action):\n",
    "    i, j = state\n",
    "    if action == ACTION_UP:\n",
    "        next_state = [max(i - 1, 0), j]\n",
    "    elif action == ACTION_LEFT:\n",
    "        next_state = [i, max(j - 1, 0)]\n",
    "    elif action == ACTION_RIGHT:\n",
    "        next_state = [i, min(j + 1, WORLD_WIDTH - 1)]\n",
    "    elif action == ACTION_DOWN:\n",
    "        next_state = [min(i + 1, WORLD_HEIGHT - 1), j]\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    reward = -1\n",
    "    if (action == ACTION_DOWN and i == 2 and 1 <= j <= 10) or (\n",
    "        action == ACTION_RIGHT and state == START):\n",
    "        reward = -100\n",
    "        next_state = START\n",
    "\n",
    "    return next_state, reward\n",
    "\n",
    "def choose_action(state, q_value):\n",
    "    if np.random.binomial(1, EPSILON) == 1:\n",
    "        return np.random.choice(ACTIONS)\n",
    "    else:\n",
    "        values_ = q_value[state[0], state[1], :]\n",
    "        return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])"
   ],
   "id": "4a5f28371d08dfa1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sarsa(q_value, expected=False, step_size=ALPHA):\n",
    "    state = START\n",
    "    action = choose_action(state, q_value)\n",
    "    rewards = 0.0\n",
    "    while state != GOAL:\n",
    "        next_state, reward = step(state, action)\n",
    "        next_action = choose_action(next_state, q_value)\n",
    "        rewards += reward\n",
    "        if not expected:\n",
    "            target = q_value[next_state[0], next_state[1], next_action]\n",
    "        else:\n",
    "            # calculate the expected value of new state\n",
    "            target = 0.0\n",
    "            q_next = q_value[next_state[0], next_state[1], :]\n",
    "            best_actions = np.argwhere(q_next == np.max(q_next))\n",
    "            for action_ in ACTIONS:\n",
    "                if action_ in best_actions:\n",
    "                    target += ((1.0 - EPSILON) / len(best_actions) + EPSILON / len(ACTIONS)) * q_value[next_state[0], next_state[1], action_]\n",
    "                else:\n",
    "                    target += EPSILON / len(ACTIONS) * q_value[next_state[0], next_state[1], action_]\n",
    "        target *= GAMMA\n",
    "        q_value[state[0], state[1], action] += step_size * (\n",
    "                reward + target - q_value[state[0], state[1], action])\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "    return rewards\n",
    "\n",
    "# an episode with Q-Learning\n",
    "# @q_value: values for state action pair, will be updated\n",
    "# @step_size: step size for updating\n",
    "# @return: total rewards within this episodegym.make('CliffWalking-v0')\n",
    "def q_learning(q_value, step_size=ALPHA):\n",
    "    state = START\n",
    "    rewards = 0.0\n",
    "    while state != GOAL:\n",
    "        action = choose_action(state, q_value)\n",
    "        next_state, reward = step(state, action)\n",
    "        rewards += reward\n",
    "        # Q-Learning update\n",
    "        q_value[state[0], state[1], action] += step_size * (\n",
    "                reward + GAMMA * np.max(q_value[next_state[0], next_state[1], :]) -\n",
    "                q_value[state[0], state[1], action])\n",
    "        state = next_state\n",
    "    return rewards\n"
   ],
   "id": "6bd82aa30868c710"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "env = gym.make('CliffWalking-v0')\n",
    "# episodes of each run\n",
    "episodes = 500\n",
    "\n",
    "# perform 40 independent runs\n",
    "runs = 50\n",
    "\n",
    "rewards_sarsa = np.zeros(episodes)\n",
    "rewards_q_learning = np.zeros(episodes)\n",
    "for r in tqdm(range(runs)):\n",
    "    q_sarsa = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
    "    q_q_learning = np.copy(q_sarsa)\n",
    "    for i in range(0, episodes):\n",
    "        # cut off the value by -100 to draw the figure more elegantly\n",
    "        # rewards_sarsa[i] += max(sarsa(q_sarsa), -100)\n",
    "        # rewards_q_learning[i] += max(q_learning(q_q_learning), -100)\n",
    "        rewards_sarsa[i] += sarsa(q_sarsa)\n",
    "        rewards_q_learning[i] += q_learning(q_q_learning)\n",
    "\n",
    "# averaging over independt runs\n",
    "rewards_sarsa /= runs\n",
    "rewards_q_learning /= runs"
   ],
   "id": "7e89ac8cb441cb5c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

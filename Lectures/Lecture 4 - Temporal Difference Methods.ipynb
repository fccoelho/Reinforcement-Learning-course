{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Methods\n",
    "Temporal-difference is one on the most important innovations in reinforcement learning. It is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment's dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap). \n",
    "\n",
    "## TD Prediction\n",
    "The prediction problem can be described as follows: given some experience and a policy $\\pi$, estimate the value function $v_\\pi$ for the nonterminal states $S_t$ occurring in that experience. Contrary to Monte Carlo methods, TD methods update estimates at each time step instead of waiting until the end of the episode. While a simple Monte-carlo method would learn a non-stationary environment at the end of each episode:\n",
    "\n",
    "\\begin{equation}\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ G_t - V(S_t) \\right],\n",
    "\\end{equation}\n",
    "\n",
    "Where $G_t$ is the return after time t, which is known only at the end of the episode. The simplest TD method is TD(0), which is defined by the update rule, waiting only for the next time step:\n",
    "\n",
    "\\begin{align}\n",
    "V(S_t) &\\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right],\n",
    "\\end{align}\n",
    "\n",
    "$$S_t \\rightarrow S_{t+1}, R_{t+1} \\rightarrow \\ldots$$\n",
    "\n",
    "where $\\alpha$ is the step-size parameter. The quantity $R_{t+1} + \\gamma V(S_{t+1})$ is called the TD target, in contrast to MC which has $G_t$ as its target. The error in the TD update is $R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$, which is the TD error. The TD target is an estimate of the return $G_t$ and, like Monte Carlo methods. This method is called $TD(0)$, or one-step TD, and it converges to $v_\\pi$ as the number of updates approaches infinity.\n",
    "\n",
    "\\begin{align}\n",
    "v_\\pi(s) &\\doteq \\mathbb{E}_\\pi \\left[ G_t | S_t = s \\right] \\\\\n",
    "&= \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma G_{t+1} | S_t = s \\right] \\\\\n",
    "&= \\mathbb{E}_\\pi \\left[ R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s \\right].\n",
    "\\end{align}\n",
    "\n",
    "$TD$ samples from expected values $\\mathbb{E}_\\pi$ and uses the current estimate of $V$ instead of the true $v_\\pi$\n",
    "## Advantages of TD Prediction Methods\n",
    "TD methods have several advantages over Monte Carlo methods:\n",
    "- TD methods can learn before knowing the final outcome. Monte Carlo methods must wait until the end of the episode to learn.\n",
    "- TD methods can learn without the final outcome. Monte Carlo methods only learn from complete episodes.\n",
    "- TD can learn without the model of the environment's dynamics. Monte Carlo methods require a model of the environment.\n",
    "\n",
    "**Example 6.2: Random Walk**\n",
    "Run the code for this example described in page 125 of the book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dynamic Programming, Monte Carlo and Temporal Difference Learning\n",
    "The key idea of Dynamic Programming (DP) is to use the Bellman equation to decompose the value function into simpler parts, and then to iteratively solve these parts. DP methods require a complete and accurate model of the environment, which is often not available. Monte Carlo (MC) methods, on the other hand, do not require a model of the environment, and can learn directly from experience. Temporal Difference (TD) learning is a combination of the two, and can learn directly from experience like MC methods, but can also bootstrap like DP methods.\n",
    "\n",
    "## Dynamic Programming\n",
    "Considering that our goal is to find the optimal policy $\\pi_*$, we can define the optimal value function $v_*$ as:\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "v_*(s) = \\max_{a \\in \\mathcal{A}} \\left[ R(s, a) + \\gamma \\sum_{s'} p(s' \\mid s, a) v_*(s') \\right].\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We can approximate this optimal value function iteratively using the update rule:\n",
    "$$\n",
    "v_{k+1}(s) = \\sum_a \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma v_k(s') \\right].\n",
    "$$\n",
    "\n",
    "This  algorithm is called **Iterative Policy Evaluation**. It is guaranteed to converge to the true value function $v_{\\pi}$ as $k \\rightarrow \\infty$. In procedural\n",
    "form it can be written as:\n",
    "$$\n",
    "\\begin{split}\n",
    "& \\text{Initialize } v(s), \\text{ for all } s \\in \\mathcal{S}^+ \\text{ arbitrarily except that } v(\\texttt{terminal}) = 0 \\\\\n",
    "& \\text{Repeat} \\\\\n",
    "& \\quad \\Delta \\leftarrow 0 \\\\\n",
    "& \\quad \\text{For each } s \\in \\mathcal{S} \\\\\n",
    "& \\quad \\quad v \\leftarrow v(s) \\\\\n",
    "& \\quad \\quad v(s) \\leftarrow \\sum_a \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma v(s') \\right] \\\\\n",
    "& \\quad \\quad \\Delta \\leftarrow \\max(\\Delta, |v - v(s)|) \\\\\n",
    "& \\text{until } \\Delta < \\theta \\text{ (a small positive number)}\n",
    "\\end{split}\n",
    "$$       \n",
    "\n",
    "### Policy Improvement\n",
    "Once we have the value function, we can improve the policy by acting greedily with respect to it. The policy improvement theorem states that if two policies $\\pi$ and $\\pi'$ are such that $q_{\\pi}(s, \\pi'(s)) \\geq v_{\\pi}(s)$ for all $s \\in \\mathcal{S}$, then the policy $\\pi'$ must be as good as, or better than, $\\pi$. This means that if we act greedily with respect to the value function, we are guaranteed to improve the policy. The policy improvement algorithm is:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\pi'(s) = \\arg \\max_a \\left[ \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma v_*(s') \\right] \\right].\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "### Policy Iteration\n",
    "The policy iteration algorithm alternates between policy evaluation and policy improvement. It is guaranteed to converge to the optimal policy and value function. In procedural form, it can be written as:\n",
    "$$\n",
    "\\begin{split}\n",
    "& \\text{Initialize } \\pi \\text{ arbitrarily, e.g., } \\pi(s) = \\texttt{random}, \\text{ for all } s \\in \\mathcal{S} \\\\\n",
    "& \\text{Repeat} \\\\\n",
    "& \\quad \\text{Policy Evaluation} \\\\\n",
    "& \\quad \\quad \\text{Repeat} \\\\\n",
    "& \\quad \\quad \\quad \\Delta \\leftarrow 0 \\\\\n",
    "& \\quad \\quad \\quad \\text{For each } s \\in \\mathcal{S} \\\\\n",
    "& \\quad \\quad \\quad \\quad v \\leftarrow v(s) \\\\\n",
    "& \\quad \\quad \\quad \\quad v(s) \\leftarrow \\sum_a \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma v(s') \\right] \\\\\n",
    "& \\quad \\quad \\quad \\quad \\Delta \\leftarrow \\max(\\Delta, |v - v(s)|) \\\\\n",
    "& \\quad \\quad \\text{until } \\Delta < \\theta \\text{ (a small positive number)} \\\\\n",
    "& \\quad \\text{Policy Improvement} \\\\\n",
    "& \\quad \\quad \\text{policy-stable} \\leftarrow \\texttt{true} \\\\\n",
    "& \\quad \\quad \\text{For each } s \\in \\mathcal{S} \\\\\n",
    "& \\quad \\quad \\quad \\text{old-action} \\leftarrow \\pi(s) \\\\\n",
    "& \\quad \\quad \\quad \\pi(s) \\leftarrow \\arg \\max_a \\left[ \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma v(s') \\right] \\right] \\\\\n",
    "& \\quad \\quad \\quad \\text{If old-action} \\neq \\pi(s), \\text{ then policy-stable} \\leftarrow \\texttt{false} \\\\\n",
    "& \\quad \\text{until policy-stable} \\\\\n",
    "\\end{split}\n",
    "$$                             \n",
    "\n",
    "### Value Iteration\n",
    "Value iteration is a simpler algorithm that combines policy evaluation and policy improvement into a single step. It is guaranteed to converge to the optimal policy and value function. In procedural form, it can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "& \\text{Initialize } v(s), \\text{ for all } s \\in \\mathcal{S}^+ \\text{ arbitrarily except that } v(\\texttt{terminal}) = 0 \\\\\n",
    "& \\text{Repeat} \\\\\n",
    "& \\quad \\Delta \\leftarrow 0 \\\\\n",
    "& \\quad \\text{For each } s \\in \\mathcal{S} \\\\\n",
    "& \\quad \\quad v \\leftarrow v(s) \\\\\n",
    "& \\quad \\quad v(s) \\leftarrow \\max_a \\left[ \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma v(s') \\right] \\right] \\\\\n",
    "& \\quad \\quad \\Delta \\leftarrow \\max(\\Delta, |v - v(s)|) \\\\\n",
    "& \\text{until } \\Delta < \\theta \\text{ (a small positive number)} \\\\\n",
    "& \\text{Output a deterministic policy } \\pi \\text{ such that} \\\\\n",
    "& \\quad \\pi(s) = \\arg \\max_a \\left[ \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma v(s') \\right] \\right].\n",
    "\\end{split}\n",
    "$$\n",
    "## Monte Carlo Methods\n",
    "Monte Carlo methods learn directly from experience, by averaging sample returns. They do not require a model of the environment, and can learn from sample episodes. The key idea is to estimate the value function by averaging the returns that have been observed from each state. The update rule is:\n",
    "$$\n",
    "v(s) \\leftarrow v(s) + \\alpha \\left[ R + \\gamma \\max_{a'} v(s') - v(s) \\right].\n",
    "$$\n",
    "\n",
    "\n",
    "## Q-learning\n",
    "Q-learning defines an algorithm to approximate $q_*$, the *optimal action-value function*. We can write it independently of the policy being followed, and it is guaranteed to converge to $q_*$ as long as all **state-action pairs** are visited infinitely often and the policy converges in the limit to the greedy policy.\n",
    "\n",
    "\\begin{equation}\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right].\n",
    "\\end{equation}\n",
    "\n",
    "In procedural form, the algorithm is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "& \\text{Initialize } Q(s, a), \\text{ for all } s \\in \\mathcal{S}^+, a \\in \\mathcal{A}(s), \\text{ arbitrarily except that } Q(\\texttt{terminal}, \\cdot) = 0 \\\\\n",
    "& \\text{Repeat (for each episode):} \\\\\n",
    "& \\quad \\text{Initialize } S \\\\\n",
    "& \\quad \\text{Repeat (for each step of episode):} \\\\\n",
    "& \\quad \\quad \\text{Choose } A \\text{ from } S \\text{ using policy derived from } Q \\text{ (e.g., } \\epsilon\\text{-greedy)} \\\\\n",
    "& \\quad \\quad \\text{Take action } A, \\text{ observe } R, S' \\\\\n",
    "& \\quad \\quad Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left[ R + \\gamma \\max_a Q(S', a) - Q(S, A) \\right] \\\\\n",
    "& \\quad \\quad S \\leftarrow S' \\\\\n",
    "& \\quad \\text{until } S \\text{ is terminal}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Q-learning is called an **off-policy** because it directly approximates $q_*$, the optimal action-value function, independent of the policy being followed. \n",
    "\n",
    "## SARSA\n",
    "SARSA is an **on-policy** TD control algorithm. It is very similar to Q-learning, but instead of using the greedy policy to select the next action, it uses the same policy that is being learned about. The update rule is:\n",
    "\n",
    "\\begin{equation}\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right].\n",
    "\\end{equation}\n",
    "\n",
    "In procedural form, the algorithm is:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "& \\text{Initialize } Q(s, a), \\text{ for all } s \\in \\mathcal{S}^+, a \\in \\mathcal{A}(s), \\text{ arbitrarily except that } Q(\\texttt{terminal}, \\cdot) = 0 \\\\\n",
    "& \\text{Repeat (for each episode):} \\\\\n",
    "& \\quad \\text{Initialize } S \\\\\n",
    "& \\quad \\text{Choose } A \\text{ from } S \\text{ using policy derived from } Q \\text{ (e.g., } \\epsilon\\text{-greedy)} \\\\\n",
    "& \\quad \\text{Repeat (for each step of episode):} \\\\\n",
    "& \\quad \\quad \\text{Take action } A, \\text{ observe } R, S', A' \\\\\n",
    "& \\quad \\quad Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left[ R + \\gamma Q(S', A') - Q(S, A) \\right] \\\\\n",
    "& \\quad \\quad S \\leftarrow S'; A \\leftarrow A' \\\\\n",
    "& \\quad \\text{until } S \\text{ is terminal}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    " The main difference of SARSA from Q-learning is that the choice action $A'$ is based on policy $\\pi$, instead of the greedy action $\\max_a Q(S', a)$.\n",
    "**Example 6.6: Cliff Walking**\n",
    "Run this example described on page 132 of the book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected SARSA\n",
    "Expected SARSA is an alternative to Q-learning that is more robust to noise and variance in the estimates of the action values. It is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "Q(S_t, A_t) &\\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\mathbb{E}_\\pi \\left[ Q(S_{t+1}, A_{t+1}) \\mid S_{t+1} \\right] - Q(S_t, A_t) \\right]\\\\\n",
    "&\\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\sum_a \\pi(a \\mid S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \\right].\n",
    "\\end{align}\n",
    "\n",
    "The above update rule, **uses the expected value instead of the maximum value over the next state,action pairs**. This is useful when the action values are noisy, and the maximum value may not be representative of the true value of the action. The expected value is more robust to noise, and is less likely to overestimate the action values. Otherwise this rule is very similar to Q-learning, updating at every time step. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

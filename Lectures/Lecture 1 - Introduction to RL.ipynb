{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Introduction to Reinforcement Learning\n",
    "Reinforcement Learning can be thought of as a third paradigm of statistical learning, separated from Supervised (e.g., Regression models) and unsupervised learning(e.g. clustering).\n",
    "\n",
    "One of the main characteristics of Reinforcement Learning is that it consists of a goal-directed agent, which interacts with an environment. The agent can observe the state of the environment, and perform actions that change the state of the environment. The agent also receives rewards from the environment, which are used to evaluate the agent's actions. The goal of the agent is to maximize the total reward it receives from the environment. The environment is uncertain, leading the agent to try to balance exploitation (leveraging previous successful experiences) *vs* exploration(trying out new strategies), in its decisions.\n",
    "\n",
    "![rl-loop](public/rl-loop.jpg)\n",
    "\n",
    "## RL as a Markov Decision Process\n",
    "\n",
    "Whenever the agent take an action, the environment is modified as a result. Therefore, the correct choice of action must take into account indirect delayed consequences of all previous actions. Thus the environment can be thought of as a Markov Decision Process (MDP).\n",
    "\n",
    "![mdp](public/600px-Markov_Decision_Process.png)\n",
    "\n",
    "## Elements of a Reinforcement Learning Problem\n",
    "- **Policy:** A policy is a mapping from states to actions. It defines the agent's behavior at any given time. The policy can be deterministic (always choosing the same action for a given state) or stochastic (choosing actions based on a probability distribution).\n",
    "- **Reward:** A reward is a numerical signal that the agent receives from the environment after taking an action. The reward can be positive\n",
    "- **Value Function:** The value function is a mapping from states to expected rewards. It defines the expected long-term reward that the agent can receive from a given state. The value function can be used to evaluate the quality of a policy.\n",
    "- **Model of the Environment:** A model of the environment is a mapping from states and actions to next states and rewards. It defines how the environment behaves in response to the agent's actions. The model can be used to simulate the environment and predict the consequences of the agent's actions.\n",
    "\n",
    "## Dynamic Programming and Optimal Control\n",
    "The methods of dynamic programming and optimal control are used to solve problems in Reinforcement Learning.\n",
    "\n",
    "*Dynamic Programming* is a method for solving problems by breaking them down into smaller subproblems, solving those subproblems, and then combining their solutions to solve the original problem. It has been used in many fields, including robotics, computer vision, and game theory.It has been used to solve many problems in Reinforcement Learning, including the optimal control problem, which is the problem of finding the optimal policy for a given environment.\n",
    "\n",
    "*Optimal control* is the problem of finding a policy that maximizes the expected long-term reward. The optimal policy can be found using dynamic programming methods, such as value iteration and policy iteration. These methods are based on the Bellman equation, which defines the relationship between the value function and the policy.\n",
    "\n",
    "Stochastic optimal control problems can be formulated as Markov Decision Processes (MDPs). In an MDP, the environment is modeled as a set of states, actions, and rewards. The agent's goal is to find a policy that maximizes the expected long-term reward.\n",
    "\n",
    "## Example: Tic-Tac-Toe\n",
    "![Tic-tac-toe](public/tictactoe.png)\n",
    "\n",
    "Consider the game of Tic-Tac-Toe. Two players take turns placing their mark on a 3x3 grid. The first player to get three of their marks in a row (horizontally, vertically, or diagonally) wins the game. If the grid is full and no player has won, the game is a draw. The game of Tic-Tac-Toe can be thought of as a Markov Decision Process (MDP). The state of the environment is the current state of the grid. The actions of the agent are the possible moves that the agent can make. The reward of the agent is 1 if the agent wins, -1 if the agent loses, and 0 if the game is a draw. The goal of the agent is to learn a policy that maximizes the probability of winning the game.\n",
    "\n",
    "This simple game cannot be solved  properly using classical techniques, such as minimax or dynamic programming. \n",
    "\n",
    "Here is how tic-tac-toe can be formulated as a Reinforcement Learning problem:\n",
    "- The agent is the player learning to play the game.\n",
    "- The environment is the game of tic-tac-toe.\n",
    "- The state of the environment is the current state of the grid.\n",
    "- The actions of the agent are the possible moves that the agent can make.\n",
    "\n",
    "First, set up a table with a value for every possible state of the game and assign a value to each state. The value of a state is the probability of winning the game if the agent is in that state. The value of a state can be estimated using the rewards received in previous games. The agent can use the value of a state to choose the best move to make in that state. If the agent is playng  X, all states with 3 Xs in a row, represent a victory and therefore are assigned a value of 1. Similarly, all states with 3 Os in a row, represent a loss and therefore are assigned a value of 0. All other states are assigned a value of 0.5, to start.\n",
    "\n",
    "The agent updates the values of the states by playing many games and estimating the probabilities of winning from each state.\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t)+\\alpha\\left[V(S_{t+1}-V(S_t)\\right]$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, and $V(S_t)$ is the value of state $S_t$. This update rule is known as temporal-difference.\n",
    "\n",
    "Consider this [code](../Code/Sutton_Barto/chapter01/tic_tac_toe.py) that learns a strategy for winning Tic-Tac-Toe, using RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Code/Sutton_Barto/chapter01/tic_tac_toe.py"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The code above trains a Tic-Tac-Toe agent using Reinforcement Learning. The agent learns to play the game by playing many games against itself and updating the values of the states based on the rewards received.\n",
    "\n",
    "Once the agent has learned a strategy for winning, it will start a game against you. You can play against the agent by using the following keys:\n",
    "\n",
    "| q | w | e |\n",
    "\n",
    "| a | s | d |\n",
    "\n",
    "| z | x | c |\n",
    "\n",
    "to place your mark on the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## A single state problem: the K-armed Bandit Problem\n",
    "The K-armed bandit problem is a simple example of a Reinforcement Learning problem. The agent is faced repeatedly with a choice. It can choose between K different actions, and receives a reward based on the action it chooses, the reward comes from a fixed probability distribution over the actions. The goal of the agent is to maximize the total reward it receives over time. The action chosen at time $t$ is denoted as $A_t$. The reward received is denoted as $R_t$. The expected value of an action $a$ is denoted as $q_*(a)$:\n",
    "\n",
    "$$q_*(a) = \\mathrm{E}[R_t \\mid A_t = a].$$\n",
    "\n",
    "The estimated value of an action $a$ at time $t$ is denoted as $Q_t(a)$. The value of an action is the expected reward given that action is selected. The value of an action is not known to the agent, and must be estimated based on the rewards received. The agent must balance exploration (trying out different actions to estimate their value) and exploitation (choosing the action with the highest estimated value). If the agent always chooses the action with the highest estimated value, we say that the agent is greedy. If the agent chooses a non-greedy action, we say that the agent is exploratory.\n",
    "\n",
    "## Action-value Methods\n",
    "Action-value methods estimate the value of each action, and select the action with the highest estimated value. The simplest action-value method is the sample-average method. The estimated value of an action $a$ at time $t$ is the average of the rewards received when $a$ was selected up to time $t$:\n",
    "\n",
    "$$Q_t(a)=\\frac{\\sum_{i=1}^{t-1} R_i \\cdot \\mathbb{1}_{A_t=a}}{\\sum_{i=1}^{t-1} \\mathbb{1}_{A_t=a}}$$\n",
    "\n",
    "The greedy action is the action with the highest estimated value:\n",
    "$$A_t=argmax_a(Q_t(a))$$\n",
    "\n",
    "A simple alternative to the greedy action is the $\\epsilon$-greedy action. With probability $\\epsilon$, the agent chooses a random action. With probability $1-\\epsilon$, the agent chooses the greedy action. The $\\epsilon$-greedy action is a simple way to balance exploration and exploitation.\n",
    "\n",
    "In the code below we run a simulation of a 10-armed bandit that learns over 1000 decisions. This experiment is repeated 2000 times for multiple values of $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true,
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%run ../Code/Sutton_Barto/chapter02/ten_armed_testbed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "To efficiently implement action-value methods, we can use the incremental update rule, to estimate the Average reward at step $n$\n",
    "$$Q_{n+1}=Q_n + \\frac{1}{n}\\left[R_n - Q_n \\right],$$\n",
    "\n",
    "where $R_n$ is the reward at step $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Nonstationary Reward Problems\n",
    "In the previous example, the reward distribution was stationary. The expected reward of each action did not change over time. In the nonstationary case, the expected reward of each action changes over time. Let $\\alpha$ be a constant step size.\n",
    "\n",
    "\\begin{align}\n",
    "Q_{n+1} &= Q_n + \\alpha \\left[R_n - Q_n \\right] \\\\\n",
    "&= \\alpha R_n + (1-\\alpha) Q_n \\\\\n",
    "&= \\alpha R_n + (1-\\alpha) \\left[ \\alpha R_{n-1} + (1-\\alpha) Q_{n-1} \\right] \\\\\n",
    "&= \\alpha R_n + (1-\\alpha) \\alpha R_{n-1} + (1-\\alpha)^2 Q_{n-1} \\\\\n",
    "&= \\dots\\\\\n",
    "&= (1-\\alpha)^n Q_1 + \\sum_{i=1}^n \\alpha (1-\\alpha)^{n-i} R_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Upper-Confidence-Bound Action Selection\n",
    "The $\\epsilon$-greedy action selection method is simple, but it does not take into account the uncertainty in the estimated value of each action. The Upper-Confidence-Bound (UCB) action selection method does. It chooses the action with the highest upper confidence bound. The upper confidence bound of an action $a$ at time $t$ is defined as:\n",
    "\\begin{align}\n",
    "A_t=argmax_a\\left[Q_t(a) +c \\sqrt{\\frac{ln\\, t}{N_t(a)}}\\right]\n",
    "\\end{align}\n",
    "\n",
    "where $ln\\, t$ is the natural logarithm of $t$ and $N_t(a)$ is the number of times action $a$ was selected up to time $t$. The constant $c > 0$ controls the degree of exploration. The UCB action selection method is more complex than the $\\epsilon$-greedy method, but it is more efficient in the long run.\n",
    "\n",
    "There are other action selection methods that have been proposed for this problem. Refer to the  Chapter 2 of the Sutton & Barto book for more details.\n",
    "\n",
    "In the code directory, you will find a [notebook](../Code/Sutton_Barto/chapter02/ten_armed_testbed.ipynb) that simulates the 10-armed bandit problem using the UCB action selection method. The notebook compares the performance of the UCB method with the $\\epsilon$-greedy method, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T19:18:47.956126107Z",
     "start_time": "2023-06-15T19:18:47.897700093Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Mathematical Notation\n",
    "\n",
    "Throughout this lecture, we have used the following mathematical notation:\n",
    "### General Notation\n",
    "- $t$ - Time step or discrete time index\n",
    "- $S_t$ - State at time $t$\n",
    "- $A_t$ - Action taken at time $t$\n",
    "- $R_t$ - Reward received at time $t$\n",
    "- $\\pi$ - Policy (mapping from states to actions)\n",
    "- $V(s)$ - Value function for state $s$\n",
    "- $Q(s,a)$ - Action-value function for state $s$ and action $a$\n",
    "\n",
    "### Bandit Problem Notation\n",
    "- $K$ - Number of arms (actions) in the bandit problem\n",
    "- $q_*(a)$ - True expected value of action $a$\n",
    "- $Q_t(a)$ - Estimated value of action $a$ at time $t$\n",
    "- $N_t(a)$ - Number of times action $a$ has been selected up to time $t$\n",
    "- $\\epsilon$ - Exploration parameter in $\\\\epsilon$-greedy action selection\n",
    "- $\\alpha$ - Learning rate or step size parameter\n",
    "- $c$ - Confidence level parameter in UCB action selection\n",
    "\n",
    "### Mathematical Operators\n",
    "- $\\mathrm{E}[\\cdot]$ - Expected value operator\n",
    "- $\\mathbb{1}_{condition}$ - Indicator function (equals 1 if condition is true, 0 otherwise)\n",
    "- $\\arg\\\\max_a f(a)$ - The argument $a$ that maximizes function $f(a)$\n",
    "- $\\ln$ - Natural logarithm\n",
    "- $\\leftarrow$ - Assignment or update operator\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

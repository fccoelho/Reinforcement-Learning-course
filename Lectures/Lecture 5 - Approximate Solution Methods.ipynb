{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Solution Methods\n",
    "In order to avoid the combinatory explosion of tabular mehods, when the number of states and/or actions is large, we must resort to approximate methods.\n",
    "Approximate methods are based on the idea of representing the value function as a parametrized function, and then updating the parameters to minimize the error between the true value function and the approximated one.\n",
    "\n",
    "## Policy Gradient Methods\n",
    "Policy gradient methods are a class of reinforcement learning algorithms that rely on estimating the gradient of the policy, and then updating the policy parameters in the direction of the gradient. Let's denote the policy parameters as $\\theta$, and the policy as $\\pi(a\\mid s,\\theta)= Pr\\{A_t=a\\mid S_t=s, \\theta_t=\\theta\\}$. We will attempt to learn $\\theta$ such that some performance measure $J(\\theta)$ is maximized. \n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta J(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is the step size.\n",
    "\n",
    "In policy gradient methods, the performance measure $J(\\theta)$ is usually the value function $v_\\pi(s)$, the state-action value function $q_\\pi(s,a)$, or some other function that is related to the value function. \n",
    "\n",
    "### Policy Gradient Theorem\n",
    "The policy gradient theorem states that the gradient of the performance measure $J(\\theta)$ is given by\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta J(\\theta) = \\sum_s \\mu_\\pi(s) \\sum_a q_\\pi(s,a) \\nabla_\\theta \\pi(a\\mid s,\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu_\\pi(s)$ is the stationary distribution of the Markov chain induced by the policy $\\pi$.\n",
    "\n",
    "### REINFORCE\n",
    "\n",
    "The policy gradient theorem can be rewriten as\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi \\left[ \\sum_a q_\\pi(s,a) \\nabla_\\theta \\pi(a\\mid s,\\theta) \\right]\n",
    "\\end{equation}\n",
    "The REINFORCE algorithm is a Monte Carlo policy gradient algorithm that uses the policy gradient theorem to update the policy parameters. A stochastic gradient ascent algorithm based on it would be: \n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t + \\alpha G_t \\sum_a q_\\pi(s,a) \\nabla_\\theta \\pi(a\\mid s,\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "where $G_t$ is the return at time $t$. This algorithm is called an \"*all actions*\" method, because its update considers all possible actions. The classical REINFORCE algorithm is an ``one action\" method, which means that it only considers the action that was actually taken. The update rule for the classical REINFORCE algorithm is\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t + \\alpha G_t \\frac{\\nabla_\\theta \\pi(A_t\\mid S_t,\\theta_t)}{\\pi(A_t\\mid S_t,\\theta_t)} \n",
    "\\end{equation}\n",
    "\n",
    "#### REINFORCE with Baseline\n",
    "The REINFORCE algorithm can be improved by subtracting a baseline from the return. The baseline is a function that does not depend on the action, and that is subtracted from the return. The baseline can be any function, but the best choice is the state value function $v_\\pi(s)$. The update rule for the REINFORCE algorithm with baseline is\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t + \\alpha (G_t - v_\\pi(S_t)) \\frac{\\nabla_\\theta \\pi(A_t\\mid S_t,\\theta_t)}{\\pi(A_t\\mid S_t,\\theta_t)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windy grid world"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:32:00.441386Z",
     "start_time": "2025-09-24T14:31:59.849093Z"
    }
   },
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "# This code was modified from the original code to better serve the purposes of our RL course at FGV.\n",
    "############################################################################################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The problem\n",
    "In this problem, we have a grid world  with a start state and a goal state. The agent can move in four directions: up, down, left, and right. The wind can blow the agent up in some columns. The agent receives a reward of -1 for each step, except when it reaches the goal state, where it receives a reward of 0. The agent can choose to move in any direction, but the wind will affect its movement. The goal is to find the optimal policy for the agent to reach the goal state in the fewest steps possible.\n",
    "\n",
    "<img src=\"windy_grid.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:32:02.237598Z",
     "start_time": "2025-09-24T14:32:02.230390Z"
    }
   },
   "source": [
    "# world height\n",
    "WORLD_HEIGHT = 7\n",
    "\n",
    "# world width\n",
    "WORLD_WIDTH = 10\n",
    "\n",
    "# wind strength for each column\n",
    "WIND = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
    "\n",
    "# possible actions\n",
    "ACTION_UP = 0\n",
    "ACTION_DOWN = 1\n",
    "ACTION_LEFT = 2\n",
    "ACTION_RIGHT = 3\n",
    "\n",
    "# probability for exploration\n",
    "EPSILON = 0.1\n",
    "\n",
    "# Sarsa step size\n",
    "ALPHA = 0.5\n",
    "\n",
    "# reward for each step\n",
    "REWARD = -1.0\n",
    "\n",
    "START = [3, 0]\n",
    "GOAL = [3, 7]\n",
    "ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implementing the SARSA algorithm\n",
    "We'll implement the SARSA algorithm to find the optimal policy for the windy grid world problem. The SARSA algorithm is an on-policy algorithm that uses the epsilon-greedy policy to explore the environment. The algorithm updates the Q-values based on the current state, action, reward, and next state and action. The goal is to find the optimal policy that maximizes the expected return in the long run."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:32:04.491768Z",
     "start_time": "2025-09-24T14:32:04.477365Z"
    }
   },
   "source": [
    "def step(state, action):\n",
    "    i, j = state\n",
    "    if action == ACTION_UP:\n",
    "        return [max(i - 1 - WIND[j], 0), j]\n",
    "    elif action == ACTION_DOWN:\n",
    "        return [max(min(i + 1 - WIND[j], WORLD_HEIGHT - 1), 0), j]\n",
    "    elif action == ACTION_LEFT:\n",
    "        return [max(i - WIND[j], 0), max(j - 1, 0)]\n",
    "    elif action == ACTION_RIGHT:\n",
    "        return [max(i - WIND[j], 0), min(j + 1, WORLD_WIDTH - 1)]\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "# play for an episode\n",
    "def episode(q_value):\n",
    "    # track the total time steps in this episode\n",
    "    time = 0\n",
    "\n",
    "    # initialize state\n",
    "    state = START\n",
    "\n",
    "    # choose an action based on epsilon-greedy algorithm\n",
    "    if np.random.binomial(1, EPSILON) == 1:\n",
    "        action = np.random.choice(ACTIONS)\n",
    "    else:\n",
    "        values_ = q_value[state[0], state[1], :]\n",
    "        action = np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])\n",
    "\n",
    "    # keep going until get to the goal state\n",
    "    while state != GOAL:\n",
    "        next_state = step(state, action)\n",
    "        if np.random.binomial(1, EPSILON) == 1:\n",
    "            next_action = np.random.choice(ACTIONS)\n",
    "        else:\n",
    "            values_ = q_value[next_state[0], next_state[1], :]\n",
    "            next_action = np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])\n",
    "\n",
    "        # Sarsa update\n",
    "        q_value[state[0], state[1], action] += \\\n",
    "            ALPHA * (REWARD + q_value[next_state[0], next_state[1], next_action] -\n",
    "                     q_value[state[0], state[1], action])\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        time += 1\n",
    "    return time"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T14:32:07.724722Z",
     "start_time": "2025-09-24T14:32:06.759203Z"
    }
   },
   "source": [
    "# Figure 6.7, 1,000 runs may be enough, # of actions in state B will also affect the curves\n",
    "def figure_6_3():\n",
    "    q_value = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
    "    episode_limit = 500\n",
    "\n",
    "    steps = []\n",
    "    ep = 0\n",
    "    while ep < episode_limit:\n",
    "        steps.append(episode(q_value))\n",
    "        # time = episode(q_value)\n",
    "        # episodes.extend([ep] * time)\n",
    "        ep += 1\n",
    "\n",
    "    steps = np.add.accumulate(steps)\n",
    "\n",
    "    plt.plot(steps, np.arange(1, len(steps) + 1))\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel('Episodes')\n",
    "\n",
    "    plt.savefig('../images/figure_6_3.png')\n",
    "    plt.close()\n",
    "\n",
    "    # display the optimal policy\n",
    "    optimal_policy = []\n",
    "    for i in range(0, WORLD_HEIGHT):\n",
    "        optimal_policy.append([])\n",
    "        for j in range(0, WORLD_WIDTH):\n",
    "            if [i, j] == GOAL:\n",
    "                optimal_policy[-1].append('G')\n",
    "                continue\n",
    "            bestAction = np.argmax(q_value[i, j, :])\n",
    "            if bestAction == ACTION_UP:\n",
    "                optimal_policy[-1].append('↑')\n",
    "            elif bestAction == ACTION_DOWN:\n",
    "                optimal_policy[-1].append('↓')\n",
    "            elif bestAction == ACTION_LEFT:\n",
    "                optimal_policy[-1].append('←')\n",
    "            elif bestAction == ACTION_RIGHT:\n",
    "                optimal_policy[-1].append('→')\n",
    "    print('Optimal policy is:')\n",
    "    for row in optimal_policy:\n",
    "        print(row)\n",
    "    print('Wind strength for each column:\\n{}'.format([str(w) for w in WIND]))\n",
    "    \n",
    "figure_6_3()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy is:\n",
      "['←', '→', '↓', '→', '→', '→', '→', '→', '→', '↓']\n",
      "['→', '→', '→', '→', '→', '→', '→', '→', '→', '↓']\n",
      "['→', '→', '↑', '→', '→', '→', '↑', '←', '→', '↓']\n",
      "['→', '→', '→', '→', '→', '→', '→', 'G', '→', '↓']\n",
      "['→', '→', '→', '→', '↑', '→', '↑', '↓', '←', '←']\n",
      "['↓', '→', '→', '→', '→', '↑', '↑', '↓', '←', '↓']\n",
      "['→', '→', '→', '→', '↑', '↑', '↑', '↑', '←', '←']\n",
      "Wind strength for each column:\n",
      "['0', '0', '0', '1', '1', '1', '2', '2', '1', '0']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

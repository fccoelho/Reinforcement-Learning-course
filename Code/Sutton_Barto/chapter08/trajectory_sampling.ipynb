{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 actions\n",
    "ACTIONS = [0, 1]\n",
    "\n",
    "# each transition has a probability to terminate with 0\n",
    "TERMINATION_PROB = 0.1\n",
    "\n",
    "# maximum expected updates\n",
    "MAX_STEPS = 20000\n",
    "\n",
    "# epsilon greedy for behavior policy\n",
    "EPSILON = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break tie randomly\n",
    "def argmax(value):\n",
    "    max_q = np.max(value)\n",
    "    return np.random.choice([a for a, q in enumerate(value) if q == max_q])\n",
    "\n",
    "\n",
    "class Task:\n",
    "    # @n_states: number of non-terminal states\n",
    "    # @b: branch\n",
    "    # Each episode starts with state 0, and state n_states is a terminal state\n",
    "    def __init__(self, n_states, b):\n",
    "        self.n_states = n_states\n",
    "        self.b = b\n",
    "\n",
    "        # transition matrix, each state-action pair leads to b possible states\n",
    "        self.transition = np.random.randint(n_states, size=(n_states, len(ACTIONS), b))\n",
    "\n",
    "        # it is not clear how to set the reward, I use a unit normal distribution here\n",
    "        # reward is determined by (s, a, s')\n",
    "        self.reward = np.random.randn(n_states, len(ACTIONS), b)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        if np.random.rand() < TERMINATION_PROB:\n",
    "            return self.n_states, 0\n",
    "        next_ = np.random.randint(self.b)\n",
    "        return self.transition[state, action, next_], self.reward[state, action, next_]\n",
    "\n",
    "\n",
    "# Evaluate the value of the start state for the greedy policy\n",
    "# derived from @q under the MDP @task\n",
    "def evaluate_pi(q, task):\n",
    "    # use Monte Carlo method to estimate the state value\n",
    "    runs = 1000\n",
    "    returns = []\n",
    "    for r in range(runs):\n",
    "        rewards = 0\n",
    "        state = 0\n",
    "        while state < task.n_states:\n",
    "            action = argmax(q[state])\n",
    "            state, r = task.step(state, action)\n",
    "            rewards += r\n",
    "        returns.append(rewards)\n",
    "    return np.mean(returns)\n",
    "\n",
    "\n",
    "# perform expected update from a uniform state-action distribution of the MDP @task\n",
    "# evaluate the learned q value every @eval_interval steps\n",
    "def uniform(task, eval_interval):\n",
    "    performance = []\n",
    "    q = np.zeros((task.n_states, 2))\n",
    "    for step in tqdm(range(MAX_STEPS)):\n",
    "        state = step // len(ACTIONS) % task.n_states\n",
    "        action = step % len(ACTIONS)\n",
    "\n",
    "        next_states = task.transition[state, action]\n",
    "        q[state, action] = (1 - TERMINATION_PROB) * np.mean(\n",
    "            task.reward[state, action] + np.max(q[next_states, :], axis=1))\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            v_pi = evaluate_pi(q, task)\n",
    "            performance.append([step, v_pi])\n",
    "\n",
    "    return zip(*performance)\n",
    "\n",
    "\n",
    "# perform expected update from an on-policy distribution of the MDP @task\n",
    "# evaluate the learned q value every @eval_interval steps\n",
    "def on_policy(task, eval_interval):\n",
    "    performance = []\n",
    "    q = np.zeros((task.n_states, 2))\n",
    "    state = 0\n",
    "    for step in tqdm(range(MAX_STEPS)):\n",
    "        if np.random.rand() < EPSILON:\n",
    "            action = np.random.choice(ACTIONS)\n",
    "        else:\n",
    "            action = argmax(q[state])\n",
    "\n",
    "        next_state, _ = task.step(state, action)\n",
    "\n",
    "        next_states = task.transition[state, action]\n",
    "        q[state, action] = (1 - TERMINATION_PROB) * np.mean(\n",
    "            task.reward[state, action] + np.max(q[next_states, :], axis=1))\n",
    "\n",
    "        if next_state == task.n_states:\n",
    "            next_state = 0\n",
    "        state = next_state\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            v_pi = evaluate_pi(q, task)\n",
    "            performance.append([step, v_pi])\n",
    "\n",
    "    return zip(*performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_8_8():\n",
    "    num_states = [1000, 10000]\n",
    "    branch = [1, 3, 10]\n",
    "    methods = [on_policy, uniform]\n",
    "\n",
    "    # average across 30 tasks\n",
    "    n_tasks = 30\n",
    "\n",
    "    # number of evaluation points\n",
    "    x_ticks = 100\n",
    "\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    for i, n in enumerate(num_states):\n",
    "        plt.subplot(2, 1, i+1)\n",
    "        for b in branch:\n",
    "            tasks = [Task(n, b) for _ in range(n_tasks)]\n",
    "            for method in methods:\n",
    "                steps = None\n",
    "                value = []\n",
    "                for task in tasks:\n",
    "                    steps, v = method(task, MAX_STEPS / x_ticks)\n",
    "                    value.append(v)\n",
    "                value = np.mean(np.asarray(value), axis=0)\n",
    "                plt.plot(steps, value, label=f'b = {b}, {method.__name__}')\n",
    "        plt.title(f'{n} states')\n",
    "\n",
    "        plt.ylabel('value of start state')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('computation time, in expected updates')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "figure_8_8()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
